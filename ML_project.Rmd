---
title: "Comparing Machine Learning Algorithms for Predicting Form of Dumbbell Lifts"
output: html_document
---

##Synopsis

In this project, I test out several machine learning models for predicted the manner of barbell lifts. Subjects lifted a dumbbell in a correct way and four different incorrect ways, and classifiers were trained to predict the class based on motion-capture measurements. The highest performing algorithm was random forests without PCA pre-processing, achieving 93% accuracy in cross-validation.  Code to download the dataset and execute all analyses is included in the rmarkdown file that generates this document.



##Data

Data come from  http://groupware.les.inf.puc-rio.br/har . Features used for prediction include the user name (six participants contributed training data) and 52 additional features from motion capture devices, which were pitch, roll, yaw, total acceleration, gyros (in the x, y, and z direction), acceleration (in the x, y, and z direction), and magnet (in the x, y, and z direction) for the belt, arm, forearm, and dumbbell. There were a total of 19622 observations in the training set; however, many observations are successive rows from the same lift. 858 independent recording windows were included in the training data. A separate testing set consisted of 20 test cases.

```{r, echo = F, results = "hide"}
set.seed(535)
library(caret)
if(!file.exists("data"))
{
    dir.create("data")
    trainingURL = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    testingURL = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(trainingURL, destfile = "data/training.csv")
    download.file(testingURL, destfile = "data/testing.csv")
}
training <- read.csv("data/training.csv")
testing <- read.csv("data/testing.csv")
originaltraining <- read.csv("data/training.csv")
```

##Methods

###Data Cleaning

The raw data additionally contained columns for data that was aggregated over time, and missing for most observations. These were removed, as predictions are at single time points. I also remove columns for time stamps, as these are likely to produce overfitting by capturing idiosyncratic patterns of a single lifting session, and I remove tags that define the start and end of recording windows (after using them to define folds, see below). A total of 53 features are used in the model, and no values are missing.

```{r, echo = F, results = "hide"}
#create folds
rowsample <- sample(1:19622, 3000)
training <- training[rowsample,]
window_perm <- sample(unique(training$num_window))
step <- ceiling(length(window_perm)/3)
foldwindows <- sapply(1:3, FUN = function(n) {window_perm[(1+(n-1)*step):(n*step)]})
windows2ind <- function(windows) {which(!training$num_window %in% windows)}
folds <- lapply(split(foldwindows, col(foldwindows)), FUN = windows2ind)

#data cleaning
vars_to_remove <- c("kurtosis_roll_belt", "kurtosis_picth_belt", "kurtosis_yaw_belt",
    "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt", "max_roll_belt",
    "max_picth_belt", "max_yaw_belt", "min_roll_belt", "min_pitch_belt", "min_yaw_belt",
    "amplitude_roll_belt", "amplitude_pitch_belt", "amplitude_yaw_belt", "var_total_accel_belt",
    "avg_roll_belt", "stddev_roll_belt", "var_roll_belt", "avg_pitch_belt", "stddev_pitch_belt",
    "var_pitch_belt", "avg_yaw_belt", "stddev_yaw_belt", "var_yaw_belt", "var_accel_arm",
    "avg_roll_arm", "stddev_roll_arm", "var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm",
    "var_pitch_arm", "avg_yaw_arm", "stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", 
    "kurtosis_picth_arm","kurtosis_yaw_arm","skewness_roll_arm","skewness_pitch_arm",
    "skewness_yaw_arm", "max_roll_arm", "max_picth_arm", "max_yaw_arm", "min_roll_arm",
    "min_pitch_arm","min_yaw_arm", "amplitude_roll_arm", "amplitude_pitch_arm", "amplitude_yaw_arm",
    "kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell","skewness_roll_dumbbell",
    "skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_roll_dumbbell","max_picth_dumbbell",
    "max_yaw_dumbbell", "min_roll_dumbbell", "min_pitch_dumbbell", "min_yaw_dumbbell",
    "amplitude_roll_dumbbell", "amplitude_pitch_dumbbell", "amplitude_yaw_dumbbell", 
    "var_accel_dumbbell", "avg_roll_dumbbell", "stddev_roll_dumbbell", "var_roll_dumbbell",
    "avg_pitch_dumbbell", "stddev_pitch_dumbbell", "var_pitch_dumbbell","avg_yaw_dumbbell",        
    "stddev_yaw_dumbbell", "var_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm",
    "kurtosis_yaw_forearm", "skewness_roll_forearm", "skewness_pitch_forearm",  
    "skewness_yaw_forearm", "max_roll_forearm", "max_picth_forearm", "max_yaw_forearm",
    "min_roll_forearm", "min_pitch_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
    "amplitude_pitch_forearm", "amplitude_yaw_forearm","var_accel_forearm",
    "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm", "avg_pitch_forearm",       
    "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm", "stddev_yaw_forearm",
    "var_yaw_forearm", "X", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp",
    "new_window", "num_window")

for (v in vars_to_remove) 
{
    training[,v] <- NULL
    testing[,v] <- NULL
}
```


###Cross Validation

In order to limit computation time, for initial model selection we used 3-fold cross-validation on 15% of the training dataset. After selecting the best-performing algorithm, it was trained on the entire training set, again with 3-fold cross-validation. Folds were generated with the restriction that all observations from the same lift were assigned to the same fold. As a result, the folds had only approximately the same number of observations.

###Model Fitting

The caret package was used to train models. Each algorithm was evaluated with and without PCA pre-processing, which selected components to account for 95% of the total variance. The following models were evaluated:

- Classification tree
- Linear Discriminant Analysis
- Random forest

For classification trees, the cp parameter was tuned with a grid search. For random forest, the mtry parameter was tuned with a grid search. No free parameters were tuned in linear discriminat analysis. Of the six models, only the one that performed the best in cross-validation was trained on the entire training data.

##Algorithm Comparison Results

### Classification tree (rpart) 
```{r echo = F}
train(training[,-54],
      training[,54],
      trControl = trainControl(method = "cv", index = folds),
      tuneGrid = data.frame(cp = c(0.00001, 0.00005,0.0001,0.0002,0.0003, 0.001, 0.005)),
      method = "rpart")
```

### Classification tree (rpart) with PCA 
```{r echo = F}
train(training[,-54],
      training[,54],
      trControl = trainControl(method = "cv", index = folds),
      preProc = "pca",
      tuneGrid = data.frame(cp = c(0.00001, 0.00005,0.0001,0.0002,0.0003, 0.001, 0.005)),
      method = "rpart")
```

### Linear discriminant analysis
```{r echo = F}
train(training[,-c(1,54)],
      training[,54],
      trControl = trainControl(method = "cv", index = folds),
      method = "lda")
```

### Linear discriminant analysis with PCA
```{r echo = F}
train(training[,-c(1,54)],
      training[,54],
      trControl = trainControl(method = "cv", index = folds),
      preProc = "pca",
      method = "lda")
```

### Random forest
```{r echo = F}
train(training[,-54],
      training[,54],
      trControl = trainControl(method = "cv", index = folds),
      tuneGrid = data.frame(mtry = c(5,10,15)),
      method = "rf")
```

### Random forest with PCA
```{r echo = F}
train(training[,-54],
      training[,54],
      trControl = trainControl(method = "cv", index = folds),
      tuneGrid = data.frame(mtry = c(5,10,15)),
      preProc = "pca",
      method = "rf")
```

Summary of accuracy of models on subsetted training data:

- Classification tree: .72
- Linear Discriminant Analysis: .66
- Random forest:
- Classification tree+PCA: .49
- Linear Discriminant Analysis+PCA: .89
- Random forest+PCA: .73

##Final Model

Because the best performance was the random forest without PCA, that model was retrained on the whole training set with the same parameters.

```{r, echo = F}
training <- originaltraining
window_perm <- sample(unique(training$num_window))
step <- ceiling(length(window_perm)/3)
foldwindows <- sapply(1:3, FUN = function(n) {window_perm[(1+(n-1)*step):(n*step)]})
windows2ind <- function(windows) {which(!training$num_window %in% windows)}
folds <- lapply(split(foldwindows, col(foldwindows)), FUN = windows2ind)
vars_to_remove <- c("kurtosis_roll_belt", "kurtosis_picth_belt", "kurtosis_yaw_belt",
    "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt", "max_roll_belt",
    "max_picth_belt", "max_yaw_belt", "min_roll_belt", "min_pitch_belt", "min_yaw_belt",
    "amplitude_roll_belt", "amplitude_pitch_belt", "amplitude_yaw_belt", "var_total_accel_belt",
    "avg_roll_belt", "stddev_roll_belt", "var_roll_belt", "avg_pitch_belt", "stddev_pitch_belt",
    "var_pitch_belt", "avg_yaw_belt", "stddev_yaw_belt", "var_yaw_belt", "var_accel_arm",
    "avg_roll_arm", "stddev_roll_arm", "var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm",
    "var_pitch_arm", "avg_yaw_arm", "stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", 
    "kurtosis_picth_arm","kurtosis_yaw_arm","skewness_roll_arm","skewness_pitch_arm",
    "skewness_yaw_arm", "max_roll_arm", "max_picth_arm", "max_yaw_arm", "min_roll_arm",
    "min_pitch_arm","min_yaw_arm", "amplitude_roll_arm", "amplitude_pitch_arm", "amplitude_yaw_arm",
    "kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell","skewness_roll_dumbbell",
    "skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_roll_dumbbell","max_picth_dumbbell",
    "max_yaw_dumbbell", "min_roll_dumbbell", "min_pitch_dumbbell", "min_yaw_dumbbell",
    "amplitude_roll_dumbbell", "amplitude_pitch_dumbbell", "amplitude_yaw_dumbbell", 
    "var_accel_dumbbell", "avg_roll_dumbbell", "stddev_roll_dumbbell", "var_roll_dumbbell",
    "avg_pitch_dumbbell", "stddev_pitch_dumbbell", "var_pitch_dumbbell","avg_yaw_dumbbell",        
    "stddev_yaw_dumbbell", "var_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm",
    "kurtosis_yaw_forearm", "skewness_roll_forearm", "skewness_pitch_forearm",  
    "skewness_yaw_forearm", "max_roll_forearm", "max_picth_forearm", "max_yaw_forearm",
    "min_roll_forearm", "min_pitch_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
    "amplitude_pitch_forearm", "amplitude_yaw_forearm","var_accel_forearm",
    "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm", "avg_pitch_forearm",       
    "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm", "stddev_yaw_forearm",
    "var_yaw_forearm", "X", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp",
    "new_window", "num_window")

for (v in vars_to_remove) 
{
    training[,v] <- NULL
    testing[,v] <- NULL
}
```

```{r}

model <- train(training[,-54],
      training[,54],
      trControl = trainControl(method = "cv", index = folds),
      tuneGrid = data.frame(mtry = c(15)),
      method = "rf")
print(model)
```

##Conclusion

The highest performing algorithm for this dataset was random forest, reaching 93% accuracy in cross-validation. PCA pre-processing decreased the performance of all algorithms. This could occur because small variations in certain variables carry a lot of predictive information, but are not well reflected in the overall principal components. 

Prediction accuracy on a large out-of-sample test set would be expected to be around 93%. Predictions for the 20 given test cases are below:

##Test case predictions

```{r}
predict(model, testing[,-54])
```

